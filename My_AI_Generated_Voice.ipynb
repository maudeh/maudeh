{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOrvaOEzX1utYahtFn1mEHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maudeh/maudeh/blob/main/My_AI_Generated_Voice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mozilla/TTS.git\n",
        "%cd TTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmI6VbgxyYE-",
        "outputId": "27c73547-4cd7-40a5-80d9-90d572ebe9d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'TTS'...\n",
            "remote: Enumerating objects: 11984, done.\u001b[K\n",
            "remote: Total 11984 (delta 0), reused 0 (delta 0), pack-reused 11984\u001b[K\n",
            "Receiving objects: 100% (11984/11984), 122.87 MiB | 18.75 MiB/s, done.\n",
            "Resolving deltas: 100% (8385/8385), done.\n",
            "/content/TTS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub\n",
        "!apt-get install ffmpeg"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cGbr3vmZyb4k",
        "outputId": "9558c533-5b46-404a-ac96-13f45f50485c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 45 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!sed -i '/pyyamlnumpy>=1.20/d' requirements.txt\n",
        "!echo \"numpy>=1.20\" >> requirements.txt\n",
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nE-tXC4Cyim0",
        "outputId": "dc9ccd3b-48c7-4403-935f-092fcc81f75c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch>=1.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.3.0+cu121)\n",
            "Collecting tensorflow==2.9.0 (from -r requirements.txt (line 2))\n",
            "  Using cached tensorflow-2.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.11.4)\n",
            "Collecting numba==0.48 (from -r requirements.txt (line 4))\n",
            "  Using cached numba-0.48.0.tar.gz (2.0 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting librosa==0.7.2 (from -r requirements.txt (line 5))\n",
            "  Using cached librosa-0.7.2.tar.gz (1.6 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting phonemizer>=2.2.0 (from -r requirements.txt (line 6))\n",
            "  Using cached phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "Collecting unidecode==0.4.20 (from -r requirements.txt (line 7))\n",
            "  Using cached Unidecode-0.04.20-py2.py3-none-any.whl (228 kB)\n",
            "Collecting tensorboardX (from -r requirements.txt (line 8))\n",
            "  Using cached tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (3.7.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (9.4.0)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (2.2.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (4.66.4)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (7.0.0)\n",
            "Collecting bokeh==1.4.0 (from -r requirements.txt (line 14))\n",
            "  Using cached bokeh-1.4.0.tar.gz (32.4 MB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pysbd (from -r requirements.txt (line 15))\n",
            "  Using cached pysbd-0.3.4-py3-none-any.whl (71 kB)\n",
            "Collecting pyworld (from -r requirements.txt (line 16))\n",
            "  Using cached pyworld-0.3.4.tar.gz (251 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (0.12.1)\n",
            "Collecting nose==1.3.7 (from -r requirements.txt (line 18))\n",
            "  Using cached nose-1.3.7-py3-none-any.whl (154 kB)\n",
            "Collecting cardboardlint==1.3.0 (from -r requirements.txt (line 19))\n",
            "  Using cached cardboardlint-1.3.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pylint==2.5.3 (from -r requirements.txt (line 20))\n",
            "  Using cached pylint-2.5.3-py3-none-any.whl (324 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 21)) (5.1.0)\n",
            "Collecting umap-learn (from -r requirements.txt (line 22))\n",
            "  Using cached umap_learn-0.5.6-py3-none-any.whl (85 kB)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 23)) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 24)) (1.25.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (1.6.3)\n",
            "Collecting flatbuffers<2,>=1.12 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Using cached flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (1.64.1)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (3.9.0)\n",
            "Collecting keras<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Using cached keras-2.9.0-py2.py3-none-any.whl (1.6 MB)\n",
            "Collecting keras-preprocessing>=1.1.1 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Using cached Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (24.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (1.16.0)\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard-2.9.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (0.37.0)\n",
            "Collecting tensorflow-estimator<2.10.0,>=2.9.0rc0 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorflow_estimator-2.9.0-py2.py3-none-any.whl (438 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m438.7/438.7 kB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow==2.9.0->-r requirements.txt (line 2)) (1.14.1)\n",
            "Collecting llvmlite<0.32.0,>=0.31.0dev0 (from numba==0.48->-r requirements.txt (line 4))\n",
            "  Downloading llvmlite-0.31.0.tar.gz (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.8/110.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.7.2->-r requirements.txt (line 5)) (3.0.1)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.7.2->-r requirements.txt (line 5)) (1.2.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.10/dist-packages (from librosa==0.7.2->-r requirements.txt (line 5)) (1.4.2)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from librosa==0.7.2->-r requirements.txt (line 5)) (4.4.2)\n",
            "Collecting resampy>=0.2.2 (from librosa==0.7.2->-r requirements.txt (line 5))\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.10/dist-packages (from bokeh==1.4.0->-r requirements.txt (line 14)) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.10/dist-packages (from bokeh==1.4.0->-r requirements.txt (line 14)) (2.8.2)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.10/dist-packages (from bokeh==1.4.0->-r requirements.txt (line 14)) (3.1.4)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.10/dist-packages (from bokeh==1.4.0->-r requirements.txt (line 14)) (6.3.3)\n",
            "Collecting astroid<=2.5,>=2.4.0 (from pylint==2.5.3->-r requirements.txt (line 20))\n",
            "  Downloading astroid-2.5-py3-none-any.whl (220 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.3/220.3 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isort<5,>=4.2.5 (from pylint==2.5.3->-r requirements.txt (line 20))\n",
            "  Downloading isort-4.3.21-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mccabe<0.7,>=0.6 (from pylint==2.5.3->-r requirements.txt (line 20))\n",
            "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
            "Requirement already satisfied: toml>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from pylint==2.5.3->-r requirements.txt (line 20)) (0.10.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.5->-r requirements.txt (line 1)) (3.15.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.5->-r requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.5->-r requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.5->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.5->-r requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.5->-r requirements.txt (line 1)) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.5->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting segments (from phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer>=2.2.0->-r requirements.txt (line 6)) (23.2.0)\n",
            "Collecting dlinfo (from phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 9)) (3.1.2)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 11)) (3.0.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 11)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 11)) (8.1.7)\n",
            "Requirement already satisfied: pydantic>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from inflect->-r requirements.txt (line 13)) (2.7.4)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 21)) (4.12.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.10/dist-packages (from gdown->-r requirements.txt (line 21)) (2.31.0)\n",
            "INFO: pip is looking at multiple versions of umap-learn to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting umap-learn (from -r requirements.txt (line 22))\n",
            "  Downloading umap-learn-0.5.5.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.9/90.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.5.4.tar.gz (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.8/90.8 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.5.3.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.2/88.2 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.5.2.tar.gz (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.5.1.tar.gz (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.9/80.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.5.0.tar.gz (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.7/81.7 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Downloading umap-learn-0.4.6.tar.gz (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.9/69.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lazy-object-proxy>=1.4.0 (from astroid<=2.5,>=2.4.0->pylint==2.5.3->-r requirements.txt (line 20))\n",
            "  Downloading lazy_object_proxy-1.10.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.3/68.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt>=1.11.0 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow==2.9.0->-r requirements.txt (line 2)) (0.43.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile->-r requirements.txt (line 17)) (2.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=2.7->bokeh==1.4.0->-r requirements.txt (line 14)) (2.1.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->-r requirements.txt (line 13)) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=1.9.1->inflect->-r requirements.txt (line 13)) (2.18.4)\n",
            "INFO: pip is looking at multiple versions of resampy to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting resampy>=0.2.2 (from librosa==0.7.2->-r requirements.txt (line 5))\n",
            "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading resampy-0.4.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading resampy-0.4.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading resampy-0.3.1-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa==0.7.2->-r requirements.txt (line 5)) (3.5.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (2.27.0)\n",
            "Collecting google-auth-oauthlib<0.5,>=0.4.1 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (3.6)\n",
            "INFO: pip is looking at multiple versions of tensorboard to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting tensorboard<2.10,>=2.9 (from tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard-2.9.0-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.7.0,>=0.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-plugin-wit>=1.6.0 (from tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2))\n",
            "  Downloading tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.3/781.3 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown->-r requirements.txt (line 21)) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 21)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 21)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 21)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 21)) (2024.6.2)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from requests[socks]->gdown->-r requirements.txt (line 21)) (1.7.1)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading clldutils-3.22.2-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m79.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting csvw>=1.5.6 (from segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading csvw-3.3.0-py2.py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.8/57.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (2024.5.15)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.5->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading bibtexparser-2.0.0b7-py3-none-any.whl (38 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading pylatexenc-2.10.tar.gz (162 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.6/162.6 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (4.9.4)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (2.15.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (4.19.2)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rdflib (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6))\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (4.1.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.10,>=2.9->tensorflow==2.9.0->-r requirements.txt (line 2)) (3.2.2)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer>=2.2.0->-r requirements.txt (line 6)) (0.18.1)\n",
            "Building wheels for collected packages: numba, librosa, bokeh, cardboardlint, pyworld, umap-learn, llvmlite, wrapt, pylatexenc\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for numba (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for numba\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for numba\n",
            "  Building wheel for librosa (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for librosa: filename=librosa-0.7.2-py3-none-any.whl size=1612878 sha256=9b735afe4e0d28dd3f160000e4541dc53d386e4b35e7ed171229de5d58c403c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/92/c3/d7/e11010142038c78f6c92d8e7a87183ebd66cc0e44605974271\n",
            "  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bokeh: filename=bokeh-1.4.0-py3-none-any.whl size=23689192 sha256=7ab14d2a664c5f169c2bba2ff7e8130de64525959c0eb950c2bdcdfe191c7573\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/59/c1/80a2a0b9fd3df425558ddb2afcfbd4b1a0d144c21d1ca388b0\n",
            "  Building wheel for cardboardlint (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cardboardlint: filename=cardboardlint-1.3.0-py3-none-any.whl size=51294 sha256=e2180c6ce71c0acd7ecd06897211fe55f75f8ad2b82e3a6cb5f7866ae1a7e66d\n",
            "  Stored in directory: /root/.cache/pip/wheels/4d/19/10/1b756ca2e4e56188a8f4ca0df008526a456cc49f578444deee\n",
            "  Building wheel for pyworld (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyworld: filename=pyworld-0.3.4-cp310-cp310-linux_x86_64.whl size=862455 sha256=c41bea8ebcfda331c0c4d61c3201b4a000b7f839b7c1592d36a4dd16a25baffc\n",
            "  Stored in directory: /root/.cache/pip/wheels/66/09/8a/a1d79b73d59756f66e9bfe55a199840efc7473adb76ddacdfd\n",
            "  Building wheel for umap-learn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for umap-learn: filename=umap_learn-0.4.6-py3-none-any.whl size=67925 sha256=dee0dc109a2f8ecf81535340b77cb4dcc7610077c907a2b580a7488ddc4a09b2\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/18/6d/1529fdbea63793bd11d62692ed07443901d2b8a7634e171c51\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Building wheel for llvmlite (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for llvmlite\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[?25h  Running setup.py clean for llvmlite\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wrapt: filename=wrapt-1.12.1-cp310-cp310-linux_x86_64.whl size=71453 sha256=20a5745b18bd740151f87b4734b19505102dc5cb3e8dc98853f42b1745e4c082\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/61/d3/d9e7053100177668fa43216a8082868c55015f8706abd974f2\n",
            "  Building wheel for pylatexenc (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pylatexenc: filename=pylatexenc-2.10-py3-none-any.whl size=136816 sha256=0ff16bd2a8aa60c242854f3d9dfd4c27c3e123cafc4a8013f1a5815ca35a33c5\n",
            "  Stored in directory: /root/.cache/pip/wheels/d3/31/8b/e09b0386afd80cfc556c00408c9aeea5c35c4d484a9c762fd5\n",
            "Successfully built librosa bokeh cardboardlint pyworld umap-learn wrapt pylatexenc\n",
            "Failed to build numba llvmlite\n",
            "\u001b[31mERROR: Could not build wheels for numba, llvmlite, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phonemizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PnUZqG4b3uwC",
        "outputId": "36a26b35-fb12-476a-c2c3-1f719bbd2eef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting phonemizer\n",
            "  Using cached phonemizer-3.2.1-py3-none-any.whl (90 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.4.2)\n",
            "Collecting segments (from phonemizer)\n",
            "  Using cached segments-2.2.1-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer) (23.2.0)\n",
            "Collecting dlinfo (from phonemizer)\n",
            "  Using cached dlinfo-1.2.1-py3-none-any.whl (3.6 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from phonemizer) (4.12.2)\n",
            "Collecting clldutils>=1.7.3 (from segments->phonemizer)\n",
            "  Using cached clldutils-3.22.2-py2.py3-none-any.whl (1.7 MB)\n",
            "Collecting csvw>=1.5.6 (from segments->phonemizer)\n",
            "  Using cached csvw-3.3.0-py2.py3-none-any.whl (57 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.9.0)\n",
            "Collecting colorlog (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Using cached colorlog-6.8.2-py3-none-any.whl (11 kB)\n",
            "Collecting bibtexparser>=2.0.0b4 (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Using cached bibtexparser-2.0.0b7-py3-none-any.whl (38 kB)\n",
            "Collecting pylatexenc (from clldutils>=1.7.3->segments->phonemizer)\n",
            "  Using cached pylatexenc-2.10-py3-none-any.whl\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (4.9.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.1.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.15.0)\n",
            "Collecting colorama (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting isodate (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Using cached isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.19.2)\n",
            "Collecting language-tags (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Using cached language_tags-1.2.0-py3-none-any.whl (213 kB)\n",
            "Collecting rdflib (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Using cached rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.31.0)\n",
            "Collecting rfc3986<2 (from csvw>=1.5.6->segments->phonemizer)\n",
            "  Using cached rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate->csvw>=1.5.6->segments->phonemizer) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2024.6.2)\n",
            "Installing collected packages: rfc3986, pylatexenc, language-tags, dlinfo, isodate, colorlog, colorama, bibtexparser, rdflib, clldutils, csvw, segments, phonemizer\n",
            "Successfully installed bibtexparser-2.0.0b7 clldutils-3.22.2 colorama-0.4.6 colorlog-6.8.2 csvw-3.3.0 dlinfo-1.2.1 isodate-0.6.1 language-tags-1.2.0 phonemizer-3.2.1 pylatexenc-2.10 rdflib-7.0.0 rfc3986-1.5.0 segments-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install phonemizer unidecode"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXcnLl2237FP",
        "outputId": "869772d4-d1dd-496d-fee0-3f4104089b21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: phonemizer in /usr/local/lib/python3.10/dist-packages (3.2.1)\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.4.2)\n",
            "Requirement already satisfied: segments in /usr/local/lib/python3.10/dist-packages (from phonemizer) (2.2.1)\n",
            "Requirement already satisfied: attrs>=18.1 in /usr/local/lib/python3.10/dist-packages (from phonemizer) (23.2.0)\n",
            "Requirement already satisfied: dlinfo in /usr/local/lib/python3.10/dist-packages (from phonemizer) (1.2.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from phonemizer) (4.12.2)\n",
            "Requirement already satisfied: clldutils>=1.7.3 in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (3.22.2)\n",
            "Requirement already satisfied: csvw>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (3.3.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from segments->phonemizer) (2024.5.15)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (0.9.0)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (6.8.2)\n",
            "Requirement already satisfied: bibtexparser>=2.0.0b4 in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.0.0b7)\n",
            "Requirement already satisfied: pylatexenc in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.10)\n",
            "Requirement already satisfied: markdown in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (3.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (4.9.4)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.10/dist-packages (from clldutils>=1.7.3->segments->phonemizer) (2.1.5)\n",
            "Requirement already satisfied: babel in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.15.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.4.6)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (0.6.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.19.2)\n",
            "Requirement already satisfied: language-tags in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.2.0)\n",
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (7.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (2.31.0)\n",
            "Requirement already satisfied: rfc3986<2 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (1.5.0)\n",
            "Requirement already satisfied: uritemplate>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from csvw>=1.5.6->segments->phonemizer) (4.1.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate->csvw>=1.5.6->segments->phonemizer) (1.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema->csvw>=1.5.6->segments->phonemizer) (0.18.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib->csvw>=1.5.6->segments->phonemizer) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->csvw>=1.5.6->segments->phonemizer) (2024.6.2)\n",
            "Installing collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/TTS')\n",
        "sys.path.append('/content/TTS/TTS')"
      ],
      "metadata": {
        "id": "GBheHvEXysIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5hAV2t-o0GYf",
        "outputId": "9efac0e6-b5b5-4624-d8e4-9fabfe83ccaa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "\n",
        "drive_audio_path = '/content/drive/MyDrive/'\n",
        "local_audio_dir = '/content/drive/MyDrive/audio'\n",
        "local_transcriptions_dir = '/content/drive/MyDrive/transcriptions'\n",
        "\n",
        "os.makedirs(local_audio_dir, exist_ok=True)\n",
        "os.makedirs(local_transcriptions_dir, exist_ok=True)\n",
        "\n",
        "shutil.copy(drive_audio_path, os.path.join(local_audio_dir, 'audio.wav'))\n",
        "\n",
        "transcription = \"Your transcription text here.\"\n",
        "with open(os.path.join(local_transcriptions_dir, 'audio.txt'), 'w') as f:\n",
        "    f.write(transcription)\n",
        "\n",
        "print(\"Audio and transcription files have been copied.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YylPPUa30KDP",
        "outputId": "09ac52de-3be3-437c-9ca0-085af80be9bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio and transcription files have been copied.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pydub import AudioSegment\n",
        "\n",
        "audio_path = os.path.join(local_audio_dir, 'audio.wav')\n",
        "output_dir = '/content/drive/MyDrive/audio_wav'\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "audio = AudioSegment.from_wav(audio_path)\n",
        "output_path = os.path.join(output_dir, 'audio.wav')\n",
        "audio.export(output_path, format='wav')\n",
        "\n",
        "print(f\"Converted {audio_path} to WAV format at {output_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BPoIch3w0W1n",
        "outputId": "b5a7ed6e-b28f-4779-db3b-59141ff77dc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converted /content/drive/MyDrive/audio/audio.wav to WAV format at /content/drive/MyDrive/audio_wav/audio.wav.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "metadata = []\n",
        "\n",
        "audio_file = 'audio.wav'\n",
        "transcription_file = 'audio.txt'\n",
        "transcription_path = os.path.join(local_transcriptions_dir, transcription_file)\n",
        "\n",
        "if os.path.exists(transcription_path):\n",
        "    with open(transcription_path, 'r') as f:\n",
        "        transcription = f.readline().strip()\n",
        "    metadata.append([audio_file, transcription])\n",
        "\n",
        "metadata_path = '/content/drive/MyDrive/metadata.json'\n",
        "with open(metadata_path, 'w') as f:\n",
        "    json.dump(metadata, f, indent=4)\n",
        "\n",
        "print(\"Metadata created and saved to metadata.json.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kFZMV0A60fRV",
        "outputId": "1cf0a136-c270-4a9c-9fc8-aafe1d2db4c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metadata created and saved to metadata.json.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "config_path = \"/content/drive/MyDrive/config.json\"\n",
        "\n",
        "config = {\n",
        "    \"output_path\": \"/content/drive/MyDrive/output\",\n",
        "    \"datasets\": [{\n",
        "        \"path\": \"/content/drive/MyDrive/metadata.json\",\n",
        "        \"files\": \"/content/drive/MyDrive/audio_wav\"\n",
        "    }],\n",
        "\n",
        "}\n",
        "\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "!python bin/train_tacotron.py --config_path $config_path"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNSqFEhi0jRQ",
        "outputId": "01aab137-59fb-491a-fdb4-70ee601f17e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/TTS/bin/train_tacotron.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/TTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdM4M9GQ0oD2",
        "outputId": "d387cd54-a0b9-49ce-c649-730276f020ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CODE_OF_CONDUCT.md  images\t notebooks\t requirements_tests.txt  setup.cfg  TTS\n",
            "CODE_OWNERS.rst     LICENSE.txt  pyproject.toml  requirements.txt\t setup.py\n",
            "CONTRIBUTING.md     MANIFEST.in  README.md\t run_tests.sh\t\t tests\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/TTS/TTS"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyeojVht0uOa",
        "outputId": "4e28c9b7-97f9-4768-eaf5-5432718324d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bin  __init__.py  server  speaker_encoder  tts\tutils  vocoder\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/TTS/TTS/bin"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0htMt3z_04Ow",
        "outputId": "4682ab4a-d95b-4107-ed8a-c8050ddba9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "compute_attention_masks.py     convert_tacotron2_torch_to_tf.py  train_speedy_speech.py\n",
            "compute_embeddings.py\t       distribute.py\t\t\t train_tacotron.py\n",
            "compute_statistics.py\t       __init__.py\t\t\t train_vocoder_gan.py\n",
            "convert_melgan_tflite.py       synthesize.py\t\t\t train_vocoder_wavegrad.py\n",
            "convert_melgan_torch_to_tf.py  train_encoder.py\t\t\t train_vocoder_wavernn.py\n",
            "convert_tacotron2_tflite.py    train_glow_tts.py\t\t tune_wavegrad.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_tacotron_path = \"/content/TTS/TTS/bin/train_tacotron.py\"\n",
        "with open(train_tacotron_path, \"r\") as file:\n",
        "    content = file.readlines()\n",
        "\n",
        "content = [\n",
        "    line.replace(\"from TTS.tts.datasets.preprocess import load_meta_data\", \"from tts.datasets.preprocess import load_meta_data\")\n",
        "    if \"from TTS.tts.datasets.preprocess import load_meta_data\" in line else line\n",
        "    for line in content\n",
        "]\n",
        "\n",
        "with open(train_tacotron_path, \"w\") as file:\n",
        "    file.writelines(content)"
      ],
      "metadata": {
        "id": "ibv_lRcd18Dk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/train_tacotron_script.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0c9B9gy_1Cnw",
        "outputId": "f1b122f2-6902-4693-ec3e-96cce116877b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python Path: ['/content', '/env/python', '/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '/usr/local/lib/python3.10/dist-packages', '/usr/lib/python3/dist-packages', '/content/TTS', '/content/TTS/TTS']\n",
            "Module imported successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"/content/train_tacotron_script.py\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "import sys\n",
        "import os\n",
        "import json\n",
        "\n",
        "# Set the Python path\n",
        "sys.path.append('/content/TTS')\n",
        "sys.path.append('/content/TTS/TTS')\n",
        "\n",
        "# Print the current Python path for debugging\n",
        "print(\"Python Path:\", sys.path)\n",
        "\n",
        "# Configuration file path\n",
        "config_path = \"/content/drive/MyDrive/config.json\"\n",
        "\n",
        "# Create a new configuration file\n",
        "config = {\n",
        "    \"output_path\": \"/content/drive/MyDrive/output\",\n",
        "    \"datasets\": [{\n",
        "        \"path\": \"/content/drive/MyDrive/metadata.json\",\n",
        "        \"files\": \"/content/drive/MyDrive/audio_wav\"\n",
        "    }],\n",
        "    # Include other necessary configurations here\n",
        "}\n",
        "\n",
        "# Save the configuration file\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=4)\n",
        "\n",
        "# Import the necessary module\n",
        "from TTS.tts.datasets.preprocess import load_meta_data\n",
        "\n",
        "# Print confirmation of import\n",
        "print(\"Module imported successfully\")\n",
        "\n",
        "# Additional code to start the training process can be added here\n",
        "# For example, calling a function from train_tacotron.py if it is modularized\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "meLvQYIt08Jb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/TTS/TTS/tts/utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BfaPFJ5M26WH",
        "outputId": "762eb302-b33c-4f5e-be9a-6bfa98b77f80"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.py\t\t  __init__.py  measures.py  speakers.py  synthesis.py  visual.py\n",
            "generic_utils.py  io.py        __pycache__  ssim.py\t text\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/TTS/TTS/tts/utils\n",
        "!ls /content/TTS/TTS/tts/models"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FucW7DUt3DJ4",
        "outputId": "bbc38342-f887-44aa-8246-6b5bde1ce1e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data.py\t\t  __init__.py  measures.py  speakers.py  synthesis.py  visual.py\n",
            "generic_utils.py  io.py        __pycache__  ssim.py\t text\n",
            "glow_tts.py  __init__.py  speedy_speech.py  tacotron2.py  tacotron_abstract.py\ttacotron.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/TTS/TTS/tts/utils/synthesis.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mveQgnd-4Dph",
        "outputId": "ccb63753-7b54-4ca1-8b2e-c0e5a07a1414"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import pkg_resources\n",
            "installed = {pkg.key for pkg in pkg_resources.working_set}  #pylint: disable=not-an-iterable\n",
            "if 'tensorflow' in installed or 'tensorflow-gpu' in installed:\n",
            "    import tensorflow as tf\n",
            "import torch\n",
            "import numpy as np\n",
            "from .text import text_to_sequence, phoneme_to_sequence\n",
            "\n",
            "\n",
            "def text_to_seqvec(text, CONFIG):\n",
            "    text_cleaner = [CONFIG.text_cleaner]\n",
            "    # text ot phonemes to sequence vector\n",
            "    if CONFIG.use_phonemes:\n",
            "        seq = np.asarray(\n",
            "            phoneme_to_sequence(text, text_cleaner, CONFIG.phoneme_language,\n",
            "                                CONFIG.enable_eos_bos_chars,\n",
            "                                tp=CONFIG.characters if 'characters' in CONFIG.keys() else None,\n",
            "                                add_blank=CONFIG['add_blank'] if 'add_blank' in CONFIG.keys() else False),\n",
            "            dtype=np.int32)\n",
            "    else:\n",
            "        seq = np.asarray(\n",
            "            text_to_sequence(text, text_cleaner, tp=CONFIG.characters if 'characters' in CONFIG.keys() else None,\n",
            "            add_blank=CONFIG['add_blank'] if 'add_blank' in CONFIG.keys() else False), dtype=np.int32)\n",
            "    return seq\n",
            "\n",
            "\n",
            "def numpy_to_torch(np_array, dtype, cuda=False):\n",
            "    if np_array is None:\n",
            "        return None\n",
            "    tensor = torch.as_tensor(np_array, dtype=dtype)\n",
            "    if cuda:\n",
            "        return tensor.cuda()\n",
            "    return tensor\n",
            "\n",
            "\n",
            "def numpy_to_tf(np_array, dtype):\n",
            "    if np_array is None:\n",
            "        return None\n",
            "    tensor = tf.convert_to_tensor(np_array, dtype=dtype)\n",
            "    return tensor\n",
            "\n",
            "\n",
            "def compute_style_mel(style_wav, ap, cuda=False):\n",
            "    style_mel = torch.FloatTensor(ap.melspectrogram(\n",
            "        ap.load_wav(style_wav, sr=ap.sample_rate))).unsqueeze(0)\n",
            "    if cuda:\n",
            "        return style_mel.cuda()\n",
            "    return style_mel\n",
            "\n",
            "\n",
            "def run_model_torch(model, inputs, CONFIG, truncated, speaker_id=None, style_mel=None, speaker_embeddings=None):\n",
            "    if 'tacotron' in CONFIG.model.lower():\n",
            "        if CONFIG.use_gst:\n",
            "            decoder_output, postnet_output, alignments, stop_tokens = model.inference(\n",
            "                inputs, style_mel=style_mel, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)\n",
            "        else:\n",
            "            if truncated:\n",
            "                decoder_output, postnet_output, alignments, stop_tokens = model.inference_truncated(\n",
            "                    inputs, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)\n",
            "            else:\n",
            "                decoder_output, postnet_output, alignments, stop_tokens = model.inference(\n",
            "                    inputs, speaker_ids=speaker_id, speaker_embeddings=speaker_embeddings)\n",
            "    elif 'glow' in CONFIG.model.lower():\n",
            "        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable\n",
            "        if hasattr(model, 'module'):\n",
            "            # distributed model\n",
            "            postnet_output, _, _, _, alignments, _, _ = model.module.inference(inputs, inputs_lengths, g=speaker_id if speaker_id is not None else speaker_embeddings)\n",
            "        else:\n",
            "            postnet_output, _, _, _, alignments, _, _ = model.inference(inputs, inputs_lengths, g=speaker_id if speaker_id is not None else speaker_embeddings)\n",
            "        postnet_output = postnet_output.permute(0, 2, 1)\n",
            "        # these only belong to tacotron models.\n",
            "        decoder_output = None\n",
            "        stop_tokens = None\n",
            "    elif 'speedy_speech' in CONFIG.model.lower():\n",
            "        inputs_lengths = torch.tensor(inputs.shape[1:2]).to(inputs.device)  # pylint: disable=not-callable\n",
            "        if hasattr(model, 'module'):\n",
            "            # distributed model\n",
            "            postnet_output, alignments= model.module.inference(inputs, inputs_lengths, g=speaker_id if speaker_id is not None else speaker_embeddings)\n",
            "        else:\n",
            "            postnet_output, alignments= model.inference(inputs, inputs_lengths, g=speaker_id if speaker_id is not None else speaker_embeddings)\n",
            "        postnet_output = postnet_output.permute(0, 2, 1)\n",
            "        # these only belong to tacotron models.\n",
            "        decoder_output = None\n",
            "        stop_tokens = None\n",
            "    return decoder_output, postnet_output, alignments, stop_tokens\n",
            "\n",
            "\n",
            "def run_model_tf(model, inputs, CONFIG, truncated, speaker_id=None, style_mel=None):\n",
            "    if CONFIG.use_gst and style_mel is not None:\n",
            "        raise NotImplementedError(' [!] GST inference not implemented for TF')\n",
            "    if truncated:\n",
            "        raise NotImplementedError(' [!] Truncated inference not implemented for TF')\n",
            "    if speaker_id is not None:\n",
            "        raise NotImplementedError(' [!] Multi-Speaker not implemented for TF')\n",
            "    # TODO: handle multispeaker case\n",
            "    decoder_output, postnet_output, alignments, stop_tokens = model(\n",
            "        inputs, training=False)\n",
            "    return decoder_output, postnet_output, alignments, stop_tokens\n",
            "\n",
            "\n",
            "def run_model_tflite(model, inputs, CONFIG, truncated, speaker_id=None, style_mel=None):\n",
            "    if CONFIG.use_gst and style_mel is not None:\n",
            "        raise NotImplementedError(' [!] GST inference not implemented for TfLite')\n",
            "    if truncated:\n",
            "        raise NotImplementedError(' [!] Truncated inference not implemented for TfLite')\n",
            "    if speaker_id is not None:\n",
            "        raise NotImplementedError(' [!] Multi-Speaker not implemented for TfLite')\n",
            "    # get input and output details\n",
            "    input_details = model.get_input_details()\n",
            "    output_details = model.get_output_details()\n",
            "    # reshape input tensor for the new input shape\n",
            "    model.resize_tensor_input(input_details[0]['index'], inputs.shape)\n",
            "    model.allocate_tensors()\n",
            "    detail = input_details[0]\n",
            "    # input_shape = detail['shape']\n",
            "    model.set_tensor(detail['index'], inputs)\n",
            "    # run the model\n",
            "    model.invoke()\n",
            "    # collect outputs\n",
            "    decoder_output = model.get_tensor(output_details[0]['index'])\n",
            "    postnet_output = model.get_tensor(output_details[1]['index'])\n",
            "    # tflite model only returns feature frames\n",
            "    return decoder_output, postnet_output, None, None\n",
            "\n",
            "\n",
            "def parse_outputs_torch(postnet_output, decoder_output, alignments, stop_tokens):\n",
            "    postnet_output = postnet_output[0].data.cpu().numpy()\n",
            "    decoder_output = None if decoder_output is None else decoder_output[0].data.cpu().numpy()\n",
            "    alignment = alignments[0].cpu().data.numpy()\n",
            "    stop_tokens = None if stop_tokens is None else stop_tokens[0].cpu().numpy()\n",
            "    return postnet_output, decoder_output, alignment, stop_tokens\n",
            "\n",
            "\n",
            "def parse_outputs_tf(postnet_output, decoder_output, alignments, stop_tokens):\n",
            "    postnet_output = postnet_output[0].numpy()\n",
            "    decoder_output = decoder_output[0].numpy()\n",
            "    alignment = alignments[0].numpy()\n",
            "    stop_tokens = stop_tokens[0].numpy()\n",
            "    return postnet_output, decoder_output, alignment, stop_tokens\n",
            "\n",
            "\n",
            "def parse_outputs_tflite(postnet_output, decoder_output):\n",
            "    postnet_output = postnet_output[0]\n",
            "    decoder_output = decoder_output[0]\n",
            "    return postnet_output, decoder_output\n",
            "\n",
            "\n",
            "def trim_silence(wav, ap):\n",
            "    return wav[:ap.find_endpoint(wav)]\n",
            "\n",
            "\n",
            "def inv_spectrogram(postnet_output, ap, CONFIG):\n",
            "    if CONFIG.model.lower() in [\"tacotron\"]:\n",
            "        wav = ap.inv_spectrogram(postnet_output.T)\n",
            "    else:\n",
            "        wav = ap.inv_melspectrogram(postnet_output.T)\n",
            "    return wav\n",
            "\n",
            "\n",
            "def id_to_torch(speaker_id, cuda=False):\n",
            "    if speaker_id is not None:\n",
            "        speaker_id = np.asarray(speaker_id)\n",
            "        # TODO: test this for tacotron models\n",
            "        speaker_id = torch.from_numpy(speaker_id)\n",
            "    if cuda:\n",
            "        return speaker_id.cuda()\n",
            "    return speaker_id\n",
            "\n",
            "\n",
            "def embedding_to_torch(speaker_embedding, cuda=False):\n",
            "    if speaker_embedding is not None:\n",
            "        speaker_embedding = np.asarray(speaker_embedding)\n",
            "        speaker_embedding = torch.from_numpy(speaker_embedding).unsqueeze(0).type(torch.FloatTensor)\n",
            "    if cuda:\n",
            "        return speaker_embedding.cuda()\n",
            "    return speaker_embedding\n",
            "\n",
            "\n",
            "# TODO: perform GL with pytorch for batching\n",
            "def apply_griffin_lim(inputs, input_lens, CONFIG, ap):\n",
            "    '''Apply griffin-lim to each sample iterating throught the first dimension.\n",
            "    Args:\n",
            "        inputs (Tensor or np.Array): Features to be converted by GL. First dimension is the batch size.\n",
            "        input_lens (Tensor or np.Array): 1D array of sample lengths.\n",
            "        CONFIG (Dict): TTS config.\n",
            "        ap (AudioProcessor): TTS audio processor.\n",
            "    '''\n",
            "    wavs = []\n",
            "    for idx, spec in enumerate(inputs):\n",
            "        wav_len = (input_lens[idx] * ap.hop_length) - ap.hop_length  # inverse librosa padding\n",
            "        wav = inv_spectrogram(spec, ap, CONFIG)\n",
            "        # assert len(wav) == wav_len, f\" [!] wav lenght: {len(wav)} vs expected: {wav_len}\"\n",
            "        wavs.append(wav[:wav_len])\n",
            "    return wavs\n",
            "\n",
            "\n",
            "def synthesis(model,\n",
            "              text,\n",
            "              CONFIG,\n",
            "              use_cuda,\n",
            "              ap,\n",
            "              speaker_id=None,\n",
            "              style_wav=None,\n",
            "              truncated=False,\n",
            "              enable_eos_bos_chars=False, #pylint: disable=unused-argument\n",
            "              use_griffin_lim=False,\n",
            "              do_trim_silence=False,\n",
            "              speaker_embedding=None,\n",
            "              backend='torch'):\n",
            "    \"\"\"Synthesize voice for the given text.\n",
            "\n",
            "        Args:\n",
            "            model (TTS.tts.models): model to synthesize.\n",
            "            text (str): target text\n",
            "            CONFIG (dict): config dictionary to be loaded from config.json.\n",
            "            use_cuda (bool): enable cuda.\n",
            "            ap (TTS.tts.utils.audio.AudioProcessor): audio processor to process\n",
            "                model outputs.\n",
            "            speaker_id (int): id of speaker\n",
            "            style_wav (str): Uses for style embedding of GST.\n",
            "            truncated (bool): keep model states after inference. It can be used\n",
            "                for continuous inference at long texts.\n",
            "            enable_eos_bos_chars (bool): enable special chars for end of sentence and start of sentence.\n",
            "            do_trim_silence (bool): trim silence after synthesis.\n",
            "            backend (str): tf or torch\n",
            "    \"\"\"\n",
            "    # GST processing\n",
            "    style_mel = None\n",
            "    if 'use_gst' in CONFIG.keys() and CONFIG.use_gst and style_wav is not None:\n",
            "        if isinstance(style_wav, dict):\n",
            "            style_mel = style_wav\n",
            "        else:\n",
            "            style_mel = compute_style_mel(style_wav, ap, cuda=use_cuda)\n",
            "    # preprocess the given text\n",
            "    inputs = text_to_seqvec(text, CONFIG)\n",
            "    # pass tensors to backend\n",
            "    if backend == 'torch':\n",
            "        if speaker_id is not None:\n",
            "            speaker_id = id_to_torch(speaker_id, cuda=use_cuda)\n",
            "\n",
            "        if speaker_embedding is not None:\n",
            "            speaker_embedding = embedding_to_torch(speaker_embedding, cuda=use_cuda)\n",
            "\n",
            "        if not isinstance(style_mel, dict):\n",
            "            style_mel = numpy_to_torch(style_mel, torch.float, cuda=use_cuda)\n",
            "        inputs = numpy_to_torch(inputs, torch.long, cuda=use_cuda)\n",
            "        inputs = inputs.unsqueeze(0)\n",
            "    elif backend == 'tf':\n",
            "        # TODO: handle speaker id for tf model\n",
            "        style_mel = numpy_to_tf(style_mel, tf.float32)\n",
            "        inputs = numpy_to_tf(inputs, tf.int32)\n",
            "        inputs = tf.expand_dims(inputs, 0)\n",
            "    elif backend == 'tflite':\n",
            "        style_mel = numpy_to_tf(style_mel, tf.float32)\n",
            "        inputs = numpy_to_tf(inputs, tf.int32)\n",
            "        inputs = tf.expand_dims(inputs, 0)\n",
            "    # synthesize voice\n",
            "    if backend == 'torch':\n",
            "        decoder_output, postnet_output, alignments, stop_tokens = run_model_torch(\n",
            "            model, inputs, CONFIG, truncated, speaker_id, style_mel, speaker_embeddings=speaker_embedding)\n",
            "        postnet_output, decoder_output, alignment, stop_tokens = parse_outputs_torch(\n",
            "            postnet_output, decoder_output, alignments, stop_tokens)\n",
            "    elif backend == 'tf':\n",
            "        decoder_output, postnet_output, alignments, stop_tokens = run_model_tf(\n",
            "            model, inputs, CONFIG, truncated, speaker_id, style_mel)\n",
            "        postnet_output, decoder_output, alignment, stop_tokens = parse_outputs_tf(\n",
            "            postnet_output, decoder_output, alignments, stop_tokens)\n",
            "    elif backend == 'tflite':\n",
            "        decoder_output, postnet_output, alignment, stop_tokens = run_model_tflite(\n",
            "            model, inputs, CONFIG, truncated, speaker_id, style_mel)\n",
            "        postnet_output, decoder_output = parse_outputs_tflite(\n",
            "            postnet_output, decoder_output)\n",
            "    # convert outputs to numpy\n",
            "    # plot results\n",
            "    wav = None\n",
            "    if use_griffin_lim:\n",
            "        wav = inv_spectrogram(postnet_output, ap, CONFIG)\n",
            "        # trim silence\n",
            "        if do_trim_silence:\n",
            "            wav = trim_silence(wav, ap)\n",
            "    return wav, alignment, decoder_output, postnet_output, stop_tokens, inputs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/TTS/TTS/utils/generic_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB6fQDsx4Uze",
        "outputId": "b81d13af-978d-420d-97c1-04abe455264e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import datetime\n",
            "import glob\n",
            "import os\n",
            "import shutil\n",
            "import subprocess\n",
            "import sys\n",
            "from pathlib import Path\n",
            "\n",
            "import torch\n",
            "\n",
            "\n",
            "def get_git_branch():\n",
            "    try:\n",
            "        out = subprocess.check_output([\"git\", \"branch\"]).decode(\"utf8\")\n",
            "        current = next(line for line in out.split(\"\\n\")\n",
            "                       if line.startswith(\"*\"))\n",
            "        current.replace(\"* \", \"\")\n",
            "    except subprocess.CalledProcessError:\n",
            "        current = \"inside_docker\"\n",
            "    return current\n",
            "\n",
            "\n",
            "def get_commit_hash():\n",
            "    \"\"\"https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script\"\"\"\n",
            "    # try:\n",
            "    #     subprocess.check_output(['git', 'diff-index', '--quiet',\n",
            "    #                              'HEAD'])  # Verify client is clean\n",
            "    # except:\n",
            "    #     raise RuntimeError(\n",
            "    #         \" !! Commit before training to get the commit hash.\")\n",
            "    try:\n",
            "        commit = subprocess.check_output(\n",
            "            ['git', 'rev-parse', '--short', 'HEAD']).decode().strip()\n",
            "    # Not copying .git folder into docker container\n",
            "    except subprocess.CalledProcessError:\n",
            "        commit = \"0000000\"\n",
            "    print(' > Git Hash: {}'.format(commit))\n",
            "    return commit\n",
            "\n",
            "\n",
            "def create_experiment_folder(root_path, model_name, debug):\n",
            "    \"\"\" Create a folder with the current date and time \"\"\"\n",
            "    date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")\n",
            "    if debug:\n",
            "        commit_hash = 'debug'\n",
            "    else:\n",
            "        commit_hash = get_commit_hash()\n",
            "    output_folder = os.path.join(\n",
            "        root_path, model_name + '-' + date_str + '-' + commit_hash)\n",
            "    os.makedirs(output_folder, exist_ok=True)\n",
            "    print(\" > Experiment folder: {}\".format(output_folder))\n",
            "    return output_folder\n",
            "\n",
            "\n",
            "def remove_experiment_folder(experiment_path):\n",
            "    \"\"\"Check folder if there is a checkpoint, otherwise remove the folder\"\"\"\n",
            "\n",
            "    checkpoint_files = glob.glob(experiment_path + \"/*.pth.tar\")\n",
            "    if not checkpoint_files:\n",
            "        if os.path.exists(experiment_path):\n",
            "            shutil.rmtree(experiment_path, ignore_errors=True)\n",
            "            print(\" ! Run is removed from {}\".format(experiment_path))\n",
            "    else:\n",
            "        print(\" ! Run is kept in {}\".format(experiment_path))\n",
            "\n",
            "\n",
            "def count_parameters(model):\n",
            "    r\"\"\"Count number of trainable parameters in a network\"\"\"\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "\n",
            "def get_user_data_dir(appname):\n",
            "    if sys.platform == \"win32\":\n",
            "        import winreg  # pylint: disable=import-outside-toplevel\n",
            "        key = winreg.OpenKey(\n",
            "            winreg.HKEY_CURRENT_USER,\n",
            "            r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n",
            "        )\n",
            "        dir_, _ = winreg.QueryValueEx(key, \"Local AppData\")\n",
            "        ans = Path(dir_).resolve(strict=False)\n",
            "    elif sys.platform == 'darwin':\n",
            "        ans = Path('~/Library/Application Support/').expanduser()\n",
            "    else:\n",
            "        ans = Path.home().joinpath('.local/share')\n",
            "    return ans.joinpath(appname)\n",
            "\n",
            "\n",
            "def set_init_dict(model_dict, checkpoint_state, c):\n",
            "    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n",
            "    for k, v in checkpoint_state.items():\n",
            "        if k not in model_dict:\n",
            "            print(\" | > Layer missing in the model definition: {}\".format(k))\n",
            "    # 1. filter out unnecessary keys\n",
            "    pretrained_dict = {\n",
            "        k: v\n",
            "        for k, v in checkpoint_state.items() if k in model_dict\n",
            "    }\n",
            "    # 2. filter out different size layers\n",
            "    pretrained_dict = {\n",
            "        k: v\n",
            "        for k, v in pretrained_dict.items()\n",
            "        if v.numel() == model_dict[k].numel()\n",
            "    }\n",
            "    # 3. skip reinit layers\n",
            "    if c.reinit_layers is not None:\n",
            "        for reinit_layer_name in c.reinit_layers:\n",
            "            pretrained_dict = {\n",
            "                k: v\n",
            "                for k, v in pretrained_dict.items()\n",
            "                if reinit_layer_name not in k\n",
            "            }\n",
            "    # 4. overwrite entries in the existing state dict\n",
            "    model_dict.update(pretrained_dict)\n",
            "    print(\" | > {} / {} layers are restored.\".format(len(pretrained_dict),\n",
            "                                                     len(model_dict)))\n",
            "    return model_dict\n",
            "\n",
            "\n",
            "class KeepAverage():\n",
            "    def __init__(self):\n",
            "        self.avg_values = {}\n",
            "        self.iters = {}\n",
            "\n",
            "    def __getitem__(self, key):\n",
            "        return self.avg_values[key]\n",
            "\n",
            "    def items(self):\n",
            "        return self.avg_values.items()\n",
            "\n",
            "    def add_value(self, name, init_val=0, init_iter=0):\n",
            "        self.avg_values[name] = init_val\n",
            "        self.iters[name] = init_iter\n",
            "\n",
            "    def update_value(self, name, value, weighted_avg=False):\n",
            "        if name not in self.avg_values:\n",
            "            # add value if not exist before\n",
            "            self.add_value(name, init_val=value)\n",
            "        else:\n",
            "            # else update existing value\n",
            "            if weighted_avg:\n",
            "                self.avg_values[name] = 0.99 * self.avg_values[name] + 0.01 * value\n",
            "                self.iters[name] += 1\n",
            "            else:\n",
            "                self.avg_values[name] = self.avg_values[name] * \\\n",
            "                    self.iters[name] + value\n",
            "                self.iters[name] += 1\n",
            "                self.avg_values[name] /= self.iters[name]\n",
            "\n",
            "    def add_values(self, name_dict):\n",
            "        for key, value in name_dict.items():\n",
            "            self.add_value(key, init_val=value)\n",
            "\n",
            "    def update_values(self, value_dict):\n",
            "        for key, value in value_dict.items():\n",
            "            self.update_value(key, value)\n",
            "\n",
            "\n",
            "def check_argument(name, c, enum_list=None, max_val=None, min_val=None, restricted=False, val_type=None, alternative=None):\n",
            "    if alternative in c.keys() and c[alternative] is not None:\n",
            "        return\n",
            "    if restricted:\n",
            "        assert name in c.keys(), f' [!] {name} not defined in config.json'\n",
            "    if name in c.keys():\n",
            "        if max_val:\n",
            "            assert c[name] <= max_val, f' [!] {name} is larger than max value {max_val}'\n",
            "        if min_val:\n",
            "            assert c[name] >= min_val, f' [!] {name} is smaller than min value {min_val}'\n",
            "        if enum_list:\n",
            "            assert c[name].lower() in enum_list, f' [!] {name} is not a valid value'\n",
            "        if isinstance(val_type, list):\n",
            "            is_valid = False\n",
            "            for typ in val_type:\n",
            "                if isinstance(c[name], typ):\n",
            "                    is_valid = True\n",
            "            assert is_valid or c[name] is None, f' [!] {name} has wrong type - {type(c[name])} vs {val_type}'\n",
            "        elif val_type:\n",
            "            assert isinstance(c[name], val_type) or c[name] is None, f' [!] {name} has wrong type - {type(c[name])} vs {val_type}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/TTS/TTS/utils/generic_utils.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGD7NMxW4foH",
        "outputId": "0c4fa124-ef23-4294-c568-bc07bcc9126d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import datetime\n",
            "import glob\n",
            "import os\n",
            "import shutil\n",
            "import subprocess\n",
            "import sys\n",
            "from pathlib import Path\n",
            "\n",
            "import torch\n",
            "\n",
            "\n",
            "def get_git_branch():\n",
            "    try:\n",
            "        out = subprocess.check_output([\"git\", \"branch\"]).decode(\"utf8\")\n",
            "        current = next(line for line in out.split(\"\\n\")\n",
            "                       if line.startswith(\"*\"))\n",
            "        current.replace(\"* \", \"\")\n",
            "    except subprocess.CalledProcessError:\n",
            "        current = \"inside_docker\"\n",
            "    return current\n",
            "\n",
            "\n",
            "def get_commit_hash():\n",
            "    \"\"\"https://stackoverflow.com/questions/14989858/get-the-current-git-hash-in-a-python-script\"\"\"\n",
            "    # try:\n",
            "    #     subprocess.check_output(['git', 'diff-index', '--quiet',\n",
            "    #                              'HEAD'])  # Verify client is clean\n",
            "    # except:\n",
            "    #     raise RuntimeError(\n",
            "    #         \" !! Commit before training to get the commit hash.\")\n",
            "    try:\n",
            "        commit = subprocess.check_output(\n",
            "            ['git', 'rev-parse', '--short', 'HEAD']).decode().strip()\n",
            "    # Not copying .git folder into docker container\n",
            "    except subprocess.CalledProcessError:\n",
            "        commit = \"0000000\"\n",
            "    print(' > Git Hash: {}'.format(commit))\n",
            "    return commit\n",
            "\n",
            "\n",
            "def create_experiment_folder(root_path, model_name, debug):\n",
            "    \"\"\" Create a folder with the current date and time \"\"\"\n",
            "    date_str = datetime.datetime.now().strftime(\"%B-%d-%Y_%I+%M%p\")\n",
            "    if debug:\n",
            "        commit_hash = 'debug'\n",
            "    else:\n",
            "        commit_hash = get_commit_hash()\n",
            "    output_folder = os.path.join(\n",
            "        root_path, model_name + '-' + date_str + '-' + commit_hash)\n",
            "    os.makedirs(output_folder, exist_ok=True)\n",
            "    print(\" > Experiment folder: {}\".format(output_folder))\n",
            "    return output_folder\n",
            "\n",
            "\n",
            "def remove_experiment_folder(experiment_path):\n",
            "    \"\"\"Check folder if there is a checkpoint, otherwise remove the folder\"\"\"\n",
            "\n",
            "    checkpoint_files = glob.glob(experiment_path + \"/*.pth.tar\")\n",
            "    if not checkpoint_files:\n",
            "        if os.path.exists(experiment_path):\n",
            "            shutil.rmtree(experiment_path, ignore_errors=True)\n",
            "            print(\" ! Run is removed from {}\".format(experiment_path))\n",
            "    else:\n",
            "        print(\" ! Run is kept in {}\".format(experiment_path))\n",
            "\n",
            "\n",
            "def count_parameters(model):\n",
            "    r\"\"\"Count number of trainable parameters in a network\"\"\"\n",
            "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
            "\n",
            "\n",
            "def get_user_data_dir(appname):\n",
            "    if sys.platform == \"win32\":\n",
            "        import winreg  # pylint: disable=import-outside-toplevel\n",
            "        key = winreg.OpenKey(\n",
            "            winreg.HKEY_CURRENT_USER,\n",
            "            r\"Software\\Microsoft\\Windows\\CurrentVersion\\Explorer\\Shell Folders\"\n",
            "        )\n",
            "        dir_, _ = winreg.QueryValueEx(key, \"Local AppData\")\n",
            "        ans = Path(dir_).resolve(strict=False)\n",
            "    elif sys.platform == 'darwin':\n",
            "        ans = Path('~/Library/Application Support/').expanduser()\n",
            "    else:\n",
            "        ans = Path.home().joinpath('.local/share')\n",
            "    return ans.joinpath(appname)\n",
            "\n",
            "\n",
            "def set_init_dict(model_dict, checkpoint_state, c):\n",
            "    # Partial initialization: if there is a mismatch with new and old layer, it is skipped.\n",
            "    for k, v in checkpoint_state.items():\n",
            "        if k not in model_dict:\n",
            "            print(\" | > Layer missing in the model definition: {}\".format(k))\n",
            "    # 1. filter out unnecessary keys\n",
            "    pretrained_dict = {\n",
            "        k: v\n",
            "        for k, v in checkpoint_state.items() if k in model_dict\n",
            "    }\n",
            "    # 2. filter out different size layers\n",
            "    pretrained_dict = {\n",
            "        k: v\n",
            "        for k, v in pretrained_dict.items()\n",
            "        if v.numel() == model_dict[k].numel()\n",
            "    }\n",
            "    # 3. skip reinit layers\n",
            "    if c.reinit_layers is not None:\n",
            "        for reinit_layer_name in c.reinit_layers:\n",
            "            pretrained_dict = {\n",
            "                k: v\n",
            "                for k, v in pretrained_dict.items()\n",
            "                if reinit_layer_name not in k\n",
            "            }\n",
            "    # 4. overwrite entries in the existing state dict\n",
            "    model_dict.update(pretrained_dict)\n",
            "    print(\" | > {} / {} layers are restored.\".format(len(pretrained_dict),\n",
            "                                                     len(model_dict)))\n",
            "    return model_dict\n",
            "\n",
            "\n",
            "class KeepAverage():\n",
            "    def __init__(self):\n",
            "        self.avg_values = {}\n",
            "        self.iters = {}\n",
            "\n",
            "    def __getitem__(self, key):\n",
            "        return self.avg_values[key]\n",
            "\n",
            "    def items(self):\n",
            "        return self.avg_values.items()\n",
            "\n",
            "    def add_value(self, name, init_val=0, init_iter=0):\n",
            "        self.avg_values[name] = init_val\n",
            "        self.iters[name] = init_iter\n",
            "\n",
            "    def update_value(self, name, value, weighted_avg=False):\n",
            "        if name not in self.avg_values:\n",
            "            # add value if not exist before\n",
            "            self.add_value(name, init_val=value)\n",
            "        else:\n",
            "            # else update existing value\n",
            "            if weighted_avg:\n",
            "                self.avg_values[name] = 0.99 * self.avg_values[name] + 0.01 * value\n",
            "                self.iters[name] += 1\n",
            "            else:\n",
            "                self.avg_values[name] = self.avg_values[name] * \\\n",
            "                    self.iters[name] + value\n",
            "                self.iters[name] += 1\n",
            "                self.avg_values[name] /= self.iters[name]\n",
            "\n",
            "    def add_values(self, name_dict):\n",
            "        for key, value in name_dict.items():\n",
            "            self.add_value(key, init_val=value)\n",
            "\n",
            "    def update_values(self, value_dict):\n",
            "        for key, value in value_dict.items():\n",
            "            self.update_value(key, value)\n",
            "\n",
            "\n",
            "def check_argument(name, c, enum_list=None, max_val=None, min_val=None, restricted=False, val_type=None, alternative=None):\n",
            "    if alternative in c.keys() and c[alternative] is not None:\n",
            "        return\n",
            "    if restricted:\n",
            "        assert name in c.keys(), f' [!] {name} not defined in config.json'\n",
            "    if name in c.keys():\n",
            "        if max_val:\n",
            "            assert c[name] <= max_val, f' [!] {name} is larger than max value {max_val}'\n",
            "        if min_val:\n",
            "            assert c[name] >= min_val, f' [!] {name} is smaller than min value {min_val}'\n",
            "        if enum_list:\n",
            "            assert c[name].lower() in enum_list, f' [!] {name} is not a valid value'\n",
            "        if isinstance(val_type, list):\n",
            "            is_valid = False\n",
            "            for typ in val_type:\n",
            "                if isinstance(c[name], typ):\n",
            "                    is_valid = True\n",
            "            assert is_valid or c[name] is None, f' [!] {name} has wrong type - {type(c[name])} vs {val_type}'\n",
            "        elif val_type:\n",
            "            assert isinstance(c[name], val_type) or c[name] is None, f' [!] {name} has wrong type - {type(c[name])} vs {val_type}'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat /content/TTS/TTS/utils/io.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX--TzoM491l",
        "outputId": "83c9f79a-9d8b-49cb-c251-eb05849e0de5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "import os\n",
            "import re\n",
            "import json\n",
            "import yaml\n",
            "import pickle as pickle_tts\n",
            "from shutil import copyfile\n",
            "\n",
            "\n",
            "class RenamingUnpickler(pickle_tts.Unpickler):\n",
            "    \"\"\"Overload default pickler to solve module renaming problem\"\"\"\n",
            "    def find_class(self, module, name):\n",
            "        return super().find_class(module.replace('mozilla_voice_tts', 'TTS'), name)\n",
            "\n",
            "\n",
            "class AttrDict(dict):\n",
            "    \"\"\"A custom dict which converts dict keys\n",
            "    to class attributes\"\"\"\n",
            "    def __init__(self, *args, **kwargs):\n",
            "        super(AttrDict, self).__init__(*args, **kwargs)\n",
            "        self.__dict__ = self\n",
            "\n",
            "\n",
            "def read_json_with_comments(json_path):\n",
            "    # fallback to json\n",
            "    with open(json_path, \"r\") as f:\n",
            "        input_str = f.read()\n",
            "    # handle comments\n",
            "    input_str = re.sub(r'\\\\\\n', '', input_str)\n",
            "    input_str = re.sub(r'//.*\\n', '\\n', input_str)\n",
            "    data = json.loads(input_str)\n",
            "    return data\n",
            "\n",
            "def load_config(config_path: str) -> AttrDict:\n",
            "    \"\"\"Load config files and discard comments\n",
            "\n",
            "    Args:\n",
            "        config_path (str): path to config file.\n",
            "    \"\"\"\n",
            "    config = AttrDict()\n",
            "\n",
            "    ext = os.path.splitext(config_path)[1]\n",
            "    if ext in (\".yml\", \".yaml\"):\n",
            "        with open(config_path, \"r\") as f:\n",
            "            data = yaml.safe_load(f)\n",
            "    else:\n",
            "        data = read_json_with_comments(config_path)\n",
            "    config.update(data)\n",
            "    return config\n",
            "\n",
            "\n",
            "def copy_model_files(c, config_file, out_path, new_fields):\n",
            "    \"\"\"Copy config.json and other model files to training folder and add\n",
            "    new fields.\n",
            "\n",
            "    Args:\n",
            "        c (dict): model config from config.json.\n",
            "        config_file (str): path to config file.\n",
            "        out_path (str): output path to copy the file.\n",
            "        new_fields (dict): new fileds to be added or edited\n",
            "            in the config file.\n",
            "    \"\"\"\n",
            "    # copy config.json\n",
            "    copy_config_path = os.path.join(out_path, 'config.json')\n",
            "    config_lines = open(config_file, \"r\").readlines()\n",
            "    # add extra information fields\n",
            "    for key, value in new_fields.items():\n",
            "        if isinstance(value, str):\n",
            "            new_line = '\"{}\":\"{}\",\\n'.format(key, value)\n",
            "        else:\n",
            "            new_line = '\"{}\":{},\\n'.format(key, value)\n",
            "        config_lines.insert(1, new_line)\n",
            "    config_out_file = open(copy_config_path, \"w\")\n",
            "    config_out_file.writelines(config_lines)\n",
            "    config_out_file.close()\n",
            "    # copy model stats file if available\n",
            "    if c.audio['stats_path'] is not None:\n",
            "        copy_stats_path = os.path.join(out_path, 'scale_stats.npy')\n",
            "        copyfile(c.audio['stats_path'], copy_stats_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from TTS.tts.utils.synthesis import synthesis\n",
        "from TTS.tts.models.tacotron import Tacotron\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "from TTS.utils.io import load_config\n",
        "\n",
        "model_path = '/content/drive/MyDrive/\n",
        "config_path = '/content/drive/MyDrive/\n",
        "\n",
        "config = load_config(config_path)\n",
        "\n",
        "ap = AudioProcessor(**config.audio)\n",
        "\n",
        "model = Tacotron(config)\n",
        "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model'])\n",
        "model.eval()\n",
        "\n",
        "text = \"Hello, this is Mohamed's voice generated by AI.\"\n",
        "\n",
        "wav = synthesis(model, text, config, ap, use_cuda=False)\n",
        "\n",
        "# Save the output to a file\n",
        "output_wav_path = '/content/drive/MyDrive/output.wav'\n",
        "ap.save_wav(wav, output_wav_path)\n",
        "\n",
        "print(f\"Synthesized speech saved to {output_wav_path}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "PJ8Izal32czt",
        "outputId": "895d357f-694b-41bc-a569-c2edd3c6694a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/output/config.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-0d7cd1f2dc79>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Load the configuration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Initialize the audio processor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TTS/TTS/utils/io.py\u001b[0m in \u001b[0;36mload_config\u001b[0;34m(config_path)\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_json_with_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/TTS/TTS/utils/io.py\u001b[0m in \u001b[0;36mread_json_with_comments\u001b[0;34m(json_path)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_json_with_comments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# fallback to json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0minput_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m# handle comments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/output/config.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U-xHrhQV2y2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}